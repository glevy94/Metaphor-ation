{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, Trainer, TrainingArguments, BartForConditionalGeneration, BartTokenizer, GenerationConfig\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "from datasets import Dataset\n",
    "\n",
    "# set GPU device\n",
    "device = \"mps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompts(sentence: str, method: str) -> str:\n",
    "    \"\"\"\n",
    "    Creates a formatted prompt for input to a generation method.\n",
    "\n",
    "    Args:\n",
    "        sentence (str): Masked sentence.\n",
    "        method (str): \"zero\" for zero-shot, \"few\" for few-shot.\n",
    "\n",
    "    Returns:\n",
    "        str: Formatted prompt for the specified method.\n",
    "    \"\"\"\n",
    "\n",
    "    # Few-shot example\n",
    "    few_shot_examples = (\n",
    "        \"Replace [MASK] to create metaphorical sentences:\\n\\n\"\n",
    "        \"Examples:\\n\"\n",
    "        \"Input: The task is [MASK] challenging.\\n\"\n",
    "        \"Output: The task is an uphill battle.\\n\\n\"\n",
    "        \"Input: She is [MASK] sad.\\n\"\n",
    "        \"Output: She is drowning in sorrow.\\n\\n\"\n",
    "        \"Return only Output:\\n\"\n",
    "    )\n",
    "\n",
    "    # Zero-shot example\n",
    "    zero_shot_example = \"Replace [MASK] to create a metaphor sentence:\"\n",
    "\n",
    "    # Create the appropriate prompt\n",
    "    if method == \"zero\":\n",
    "        prompt = f\"{zero_shot_example} {sentence}\"\n",
    "    elif method == \"few\":\n",
    "        prompt = f\"{few_shot_examples}Input: {sentence}\\nOutput:\"\n",
    "    else:\n",
    "        raise ValueError(\"Invalid method. Choose 'zero' or 'few'.\")\n",
    "\n",
    "    return prompt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_metaphors(model, tokenizer, sentences: list[str], prompt_type: str = \"few\", view_output: bool = False, ) -> list[str]: # type: ignore\n",
    "    \n",
    "    \"\"\"\n",
    "    Generates metaphor sentences from masked sentences\n",
    "\n",
    "    Args:\n",
    "        model: pre-trained model\n",
    "        tokenizer: model tokenizer\n",
    "        sentences (list): list of sentences to transform\n",
    "        prompt_type (str): \"zero\" for zero shot \"few\" for few shot\n",
    "        view_output (bool): set true to print masked sentences and generated metaphor\n",
    "        \n",
    "    Returns:\n",
    "        A list of transformed sentences\n",
    "    \"\"\"\n",
    "    \n",
    "    model = model.to(device)\n",
    "    output = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "\n",
    "        prompt = prompts(sentence, prompt_type)\n",
    "\n",
    "        # Tokenize and move to the device\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "        # Generate the output\n",
    "        output_ids = model.generate(\n",
    "            inputs=input_ids[\"input_ids\"],  \n",
    "            max_length=150,                # Allow for longer outputs\n",
    "            num_beams=10,                  \n",
    "            do_sample=True,\n",
    "            temperature=0.5,               # Slightly higher temperature for creativity\n",
    "            #top_k=50,                      # Use top-k sampling to encourage variation\n",
    "            top_p=0.95,                     # Use nucleus sampling for creativity\n",
    "            early_stopping=False\n",
    "        )\n",
    "\n",
    "        # Decode and print the output\n",
    "        output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        output.append(output_text)\n",
    "\n",
    "        if view_output is True:\n",
    "            print(f\"Input: {sentence}\\nOutput: {output_text}\\n\")\n",
    "            \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a function to load your models and your generate_metaphors function ready\n",
    "def load_model_t5(model_path):\n",
    "\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_path).to(device)\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_path, legacy=False, clean_up_tokenization_spaces=True)\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "def load_model_bart(model_path):\n",
    "\n",
    "    model = BartForConditionalGeneration.from_pretrained(model_path).to(device)\n",
    "    tokenizer = BartTokenizer.from_pretrained(model_path, clean_up_tokenization_spaces=True)\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_with_the = pd.read_csv(\"data/test_sentences_begin_the.csv\")\n",
    "sentences_without_the = pd.read_csv(\"data/test_sentences_no_the.csv\")\n",
    "\n",
    "sentences = pd.concat([sentences_with_the, sentences_without_the], ignore_index=True)\n",
    "sentences =sentences[\"synthetic_sentences\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_model_paths = [\"fine_tuned_t5_moh_x\", \"fine_tuned_t5_vua\", \"fine_tuned_t5_trofi\", \"fine_tuned_t5_combined_df\"]\n",
    "bart_model_paths = [\"fine_tuned_bart_moh_x\", \"fine_tuned_bart_vua\", \"fine_tuned_bart_trofi\", \"fine_tuned_bart_combined_df\"]\n",
    "\n",
    "# Initialize an empty list to hold results for both models\n",
    "results = []\n",
    "\n",
    "for model_path in t5_model_paths:\n",
    "    model_name = model_path\n",
    "    model, tokenizer = load_model_t5(model_path)  # Load the T5 model and tokenizer\n",
    "    \n",
    "    # Generate metaphors\n",
    "    outputs = generate_metaphors(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        sentences=sentences,\n",
    "        prompt_type=\"zero\",\n",
    "        view_output=False\n",
    "    )\n",
    "    \n",
    "    # Save results to the list\n",
    "    for sentence, output in zip(sentences, outputs):\n",
    "        results.append({\n",
    "            \"model_name\": model_name,\n",
    "            \"input_sentence\": sentence,\n",
    "            \"generated_output\": output\n",
    "        })\n",
    "\n",
    "# Loop through BART models\n",
    "for model_path in bart_model_paths:\n",
    "    model_name = model_path\n",
    "    model, tokenizer = load_model_bart(model_path)  # Load the BART model and tokenizer\n",
    "    \n",
    "    # Generate metaphors\n",
    "    outputs = generate_metaphors(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        sentences=sentences,\n",
    "        prompt_type=\"zero\",\n",
    "        view_output=False\n",
    "    )\n",
    "    \n",
    "    # Save results to the list\n",
    "    for sentence, output in zip(sentences, outputs):\n",
    "        results.append({\n",
    "            \"model_name\": model_name,\n",
    "            \"input_sentence\": sentence,\n",
    "            \"generated_output\": output\n",
    "        })\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "combined_df = pd.DataFrame(results)\n",
    "\n",
    "# Save to CSV or display\n",
    "#combined_df.to_csv(\"metaphor_outputs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.to_csv(\"metaphor_outputs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maui",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
