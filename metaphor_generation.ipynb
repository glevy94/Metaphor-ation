{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, Trainer, TrainingArguments, BartForConditionalGeneration, BartTokenizer, GenerationConfig\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "from datasets import Dataset\n",
    "\n",
    "# set GPU device\n",
    "device = \"mps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New DataFrame variables created:\n",
      "vua_df\n",
      "trofi_df\n",
      "moh_x_df\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "combined_df = pd.read_csv(\"data/combined_dataframe.csv\")\n",
    "\n",
    "dataset_names = combined_df[\"data_set\"].unique()\n",
    "\n",
    "df_names = []\n",
    "for name in dataset_names:\n",
    "\n",
    "    df_name = f\"{name}_df\"\n",
    "\n",
    "    globals()[f\"{df_name}\"] = (\n",
    "        combined_df[combined_df[\"data_set\"] == name]\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    df_names.append(df_name)\n",
    "\n",
    "\n",
    "print(\"New DataFrame variables created:\")\n",
    "for df_name in df_names:\n",
    "    print(df_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vua_df = vua_df # type: ignore\n",
    "trofi_df = trofi_df # type: ignore\n",
    "moh_x_df = moh_x_df # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data set to train on\n",
    "\n",
    "dataset = Dataset.from_pandas(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcfc499b999742aca984a3573604bc8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/22812 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "model_name = \"google/flan-t5-small\" \n",
    "model_t5 = T5ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "tokenizer_t5 = T5Tokenizer.from_pretrained(model_name, legacy=False, clean_up_tokenization_spaces=True)\n",
    "\n",
    "# Add [MASK] token if not already present\n",
    "if \"[MASK]\" not in tokenizer_t5.get_vocab():\n",
    "    tokenizer_t5.add_tokens(\"[MASK]\")\n",
    "    model_t5.resize_token_embeddings(len(tokenizer_t5))  # Resize embeddings to match the updated vocab size\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    prompt = \"Replace [MASK] to create a metaphor sentence: {masked_sentence}\"\n",
    "\n",
    "    inputs = [\n",
    "        prompt.format(masked_sentence=sentence)\n",
    "        for sentence in examples['masked_sentence']\n",
    "    ]\n",
    "    \n",
    "    targets = examples['sentence']\n",
    "    \n",
    "    model_inputs = tokenizer_t5(inputs, max_length=200, truncation=True, padding=\"max_length\")\n",
    "    labels = tokenizer_t5(targets, max_length=200, truncation=True, padding=\"max_length\")[\"input_ids\"]\n",
    "    labels = [[-100 if token == tokenizer_t5.pad_token_id else token for token in label] for label in labels]\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# dataset split to verify model works\n",
    "dataset_split = tokenized_dataset.train_test_split(test_size=0.05, seed=42)\n",
    "eval_dataset = dataset_split['test']\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./fine_tuned_flan_t5\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=3e-4,\n",
    "    per_device_train_batch_size=32,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=100,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_t5,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    eval_dataset=tokenized_dataset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "#trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_t5_combined_df/tokenizer_config.json',\n",
       " './fine_tuned_t5_combined_df/special_tokens_map.json',\n",
       " './fine_tuned_t5_combined_df/spiece.model',\n",
       " './fine_tuned_t5_combined_df/added_tokens.json')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model\n",
    "output_dir = \"./fine_tuned_t5_combined_df\" #change for each dataset trained on\n",
    "model_t5.save_pretrained(output_dir)\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer_t5.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data set to train on\n",
    "\n",
    "dataset = Dataset.from_pandas(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating mask token\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "343c7202c02a42c493ba16c8df61cd1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/22812 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "model_name = \"facebook/bart-base\"  # Use BART model\n",
    "model_bart = BartForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "tokenizer_bart = BartTokenizer.from_pretrained(model_name, clean_up_tokenization_spaces=True)\n",
    "\n",
    "# Add [MASK] token if not already present (optional for BART)\n",
    "if \"[MASK]\" not in tokenizer_bart.get_vocab():\n",
    "    print(\"updating mask token\")\n",
    "    tokenizer_bart.add_tokens(\"[MASK]\")\n",
    "    tokenizer_bart.mask_token = \"[MASK]\"\n",
    "    model_bart.resize_token_embeddings(len(tokenizer_bart))  # Resize embeddings to match updated vocab size\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    prompt = \"Replace [MASK] to create a metaphor sentence: {masked_sentence}\"\n",
    "    \n",
    "    # Prepare inputs and targets\n",
    "    inputs = [\n",
    "        prompt.format(masked_sentence=sentence)\n",
    "        for sentence in examples['masked_sentence']\n",
    "    ]\n",
    "    targets = examples['sentence']\n",
    "    \n",
    "    # Tokenize inputs and targets\n",
    "    model_inputs = tokenizer_bart(inputs, max_length=150, truncation=True, padding=\"max_length\")\n",
    "    labels = tokenizer_bart(targets, max_length=150, truncation=True, padding=\"max_length\")[\"input_ids\"]\n",
    "    labels = [[-100 if token == tokenizer_bart.pad_token_id else token for token in label] for label in labels]\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return model_inputs\n",
    "\n",
    "# tokenize dataset\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Eval set for confirming model fit\n",
    "dataset_split = tokenized_dataset.train_test_split(test_size=0.05, seed=42)\n",
    "eval_dataset = dataset_split['test']\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./fine_tuned_bart\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32,  \n",
    "    num_train_epochs=1,\n",
    "    logging_steps=100,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_bart,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "\n",
    "#trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_bart_combined_df/tokenizer_config.json',\n",
       " './fine_tuned_bart_combined_df/special_tokens_map.json',\n",
       " './fine_tuned_bart_combined_df/vocab.json',\n",
       " './fine_tuned_bart_combined_df/merges.txt',\n",
       " './fine_tuned_bart_combined_df/added_tokens.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model\n",
    "output_dir = \"./fine_tuned_bart_combined_df\" #change for each dataset trained on\n",
    "model_bart.save_pretrained(output_dir)\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer_bart.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>synthetic_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The ocean [MASK] against the cliffs, roaring i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The garden [MASK] with colors as the flowers g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The airplane [MASK] through the sky, leaving a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The dog [MASK] through the field, a blur of fu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The sky [MASK] a mosaic of pink and orange as ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 synthetic_sentences\n",
       "0  The ocean [MASK] against the cliffs, roaring i...\n",
       "1  The garden [MASK] with colors as the flowers g...\n",
       "2  The airplane [MASK] through the sky, leaving a...\n",
       "3  The dog [MASK] through the field, a blur of fu...\n",
       "4  The sky [MASK] a mosaic of pink and orange as ..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_with_the = pd.read_csv(\"data/test_sentences_begin_the.csv\")\n",
    "sentences_without_the = pd.read_csv(\"data/test_sentences_no_the.csv\")\n",
    "\n",
    "sentences_with_the.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompts(sentence: str, method: str) -> str:\n",
    "    \"\"\"\n",
    "    Creates a formatted prompt for input to a generation method.\n",
    "\n",
    "    Args:\n",
    "        sentence (str): Masked sentence.\n",
    "        method (str): \"zero\" for zero-shot, \"few\" for few-shot.\n",
    "\n",
    "    Returns:\n",
    "        str: Formatted prompt for the specified method.\n",
    "    \"\"\"\n",
    "\n",
    "    # Few-shot example\n",
    "    few_shot_examples = (\n",
    "        \"Replace [MASK] to create metaphorical sentences:\\n\\n\"\n",
    "        \"Examples:\\n\"\n",
    "        \"Input: The task is [MASK] challenging.\\n\"\n",
    "        \"Output: The task is an uphill battle.\\n\\n\"\n",
    "        \"Input: She is [MASK] sad.\\n\"\n",
    "        \"Output: She is drowning in sorrow.\\n\\n\"\n",
    "        \"Return only Output:\\n\"\n",
    "    )\n",
    "\n",
    "    # Zero-shot example\n",
    "    zero_shot_example = \"Replace [MASK] to create a metaphor sentence:\"\n",
    "\n",
    "    # Create the appropriate prompt\n",
    "    if method == \"zero\":\n",
    "        prompt = f\"{zero_shot_example} {sentence}\"\n",
    "    elif method == \"few\":\n",
    "        prompt = f\"{few_shot_examples}Input: {sentence}\\nOutput:\"\n",
    "    else:\n",
    "        raise ValueError(\"Invalid method. Choose 'zero' or 'few'.\")\n",
    "\n",
    "    return prompt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replace [MASK] to create metaphorical sentences:\n",
      "\n",
      "Examples:\n",
      "Input: The task is [MASK] challenging.\n",
      "Output: The task is an uphill battle.\n",
      "\n",
      "Input: She is [MASK] sad.\n",
      "Output: She is drowning in sorrow.\n",
      "\n",
      "Return only Output:\n",
      "Input: Testing prompt and sentence structure\n",
      "Output:\n"
     ]
    }
   ],
   "source": [
    "print(prompts(\"Testing prompt and sentence structure\", \"few\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_metaphors(model, tokenizer, sentences: list[str], prompt_type: str = \"few\", view_output: bool = False, ) -> list[str]: # type: ignore\n",
    "    \n",
    "    \"\"\"\n",
    "    Generates metaphor sentences from masked sentences\n",
    "\n",
    "    Args:\n",
    "        model: pre-trained model\n",
    "        tokenizer: model tokenizer\n",
    "        sentences (list): list of sentences to transform\n",
    "        prompt_type (str): \"zero\" for zero shot \"few\" for few shot\n",
    "        view_output (bool): set true to print masked sentences and generated metaphor\n",
    "        \n",
    "    Returns:\n",
    "        A list of transformed sentences\n",
    "    \"\"\"\n",
    "    \n",
    "    model = model.to(device)\n",
    "    output = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "\n",
    "        prompt = prompts(sentence, prompt_type)\n",
    "\n",
    "        # Tokenize and move to the device\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "        # Generate the output\n",
    "        output_ids = model.generate(\n",
    "            inputs=input_ids[\"input_ids\"],  \n",
    "            max_length=150,                # Allow for longer outputs\n",
    "            num_beams=10,                  \n",
    "            do_sample=True,\n",
    "            temperature=0.5,               # Slightly higher temperature for creativity\n",
    "            #top_k=50,                      # Use top-k sampling to encourage variation\n",
    "            top_p=0.95,                     # Use nucleus sampling for creativity\n",
    "            early_stopping=False\n",
    "        )\n",
    "\n",
    "        # Decode and print the output\n",
    "        output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        output.append(output_text)\n",
    "\n",
    "        if view_output is True:\n",
    "            print(f\"Input: {sentence}\\nOutput: {output_text}\\n\")\n",
    "            \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sentences_without_the[\"synthetic_sentences\"].tolist()[5:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Sunlight [MASK] through the branches.\n",
      "Output: Sunlightsthrough the branches.\n",
      "\n",
      "Input: Ants [MASK] across the forest floor.\n",
      "Output: Ants spit across the forest floor.\n",
      "\n",
      "Input: Flames [MASK] in the fireplace.\n",
      "Output: Flames savour in the fireplace.\n",
      "\n",
      "Input: Leaves [MASK] to the ground.\n",
      "Output: Leaves sulfate to the ground.\n",
      "\n",
      "Input: Music [MASK] from the speakers.\n",
      "Output: Music speaks from the speakers.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Sunlightsthrough the branches.',\n",
       " 'Ants spit across the forest floor.',\n",
       " 'Flames savour in the fireplace.',\n",
       " 'Leaves sulfate to the ground.',\n",
       " 'Music speaks from the speakers.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5_output = generate_metaphors(model=model_t5,tokenizer=tokenizer_t5, sentences=sentences, prompt_type=\"zero\", view_output=True)\n",
    "t5_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Sunlight [MASK] through the branches.\n",
      "Output: Replace  Wind through the branches to create a metaphor sentence: Sunlight  wind through the trees.\n",
      "\n",
      "Input: Ants [MASK] across the forest floor.\n",
      "Output: Replace  Horace  with Ants to create a metaphor sentence: Ants  walk across the forest floor.\n",
      "\n",
      "Input: Flames [MASK] in the fireplace.\n",
      "Output: Replace  the words to create a metaphor sentence: Flames  burning in the fireplace.\n",
      "\n",
      "Input: Leaves [MASK] to the ground.\n",
      "Output: Replace __________ with _____________ to create a metaphor sentence: Leaves ___________ to the ground.\n",
      "\n",
      "Input: Music [MASK] from the speakers.\n",
      "Output: Replace  the words to create a metaphor sentence: Music  comes from the speakers.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Replace  Wind through the branches to create a metaphor sentence: Sunlight  wind through the trees.',\n",
       " 'Replace  Horace  with Ants to create a metaphor sentence: Ants  walk across the forest floor.',\n",
       " 'Replace  the words to create a metaphor sentence: Flames  burning in the fireplace.',\n",
       " 'Replace __________ with _____________ to create a metaphor sentence: Leaves ___________ to the ground.',\n",
       " 'Replace  the words to create a metaphor sentence: Music  comes from the speakers.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart_output = generate_metaphors(model=model_bart,tokenizer=tokenizer_bart, sentences=sentences, prompt_type=\"zero\", view_output=True)\n",
    "bart_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maui",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
