{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, Trainer, TrainingArguments, BartForConditionalGeneration, BartTokenizer, GenerationConfig\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "from datasets import Dataset\n",
    "\n",
    "# set GPU device\n",
    "device = \"mps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New DataFrame variables created:\n",
      "vua_df\n",
      "trofi_df\n",
      "moh_x_df\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "combined_df = pd.read_csv(\"data/combined_dataframe.csv\")\n",
    "\n",
    "dataset_names = combined_df[\"data_set\"].unique()\n",
    "\n",
    "df_names = []\n",
    "for name in dataset_names:\n",
    "\n",
    "    df_name = f\"{name}_df\"\n",
    "\n",
    "    globals()[f\"{df_name}\"] = (\n",
    "        combined_df[combined_df[\"data_set\"] == name]\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    df_names.append(df_name)\n",
    "\n",
    "\n",
    "print(\"New DataFrame variables created:\")\n",
    "for df_name in df_names:\n",
    "    print(df_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vua_df = vua_df # type: ignore\n",
    "trofi_df = trofi_df # type: ignore\n",
    "moh_x_df = moh_x_df # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data set to train on\n",
    "\n",
    "dataset = Dataset.from_pandas(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfcabd4a2a7e4c49b8e27acb51a087a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/22812 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74e01274cc7444f8b94520806230437c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/713 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2131, 'grad_norm': 0.689591646194458, 'learning_rate': 0.0002579242636746143, 'epoch': 0.14}\n",
      "{'loss': 0.1454, 'grad_norm': 0.855769693851471, 'learning_rate': 0.0002158485273492286, 'epoch': 0.28}\n",
      "{'loss': 0.133, 'grad_norm': 0.3644113838672638, 'learning_rate': 0.0001737727910238429, 'epoch': 0.42}\n",
      "{'loss': 0.1169, 'grad_norm': 0.41035717725753784, 'learning_rate': 0.00013169705469845723, 'epoch': 0.56}\n",
      "{'loss': 0.1118, 'grad_norm': 0.31623443961143494, 'learning_rate': 8.962131837307153e-05, 'epoch': 0.7}\n",
      "{'loss': 0.1062, 'grad_norm': 0.3386119604110718, 'learning_rate': 4.754558204768583e-05, 'epoch': 0.84}\n",
      "{'loss': 0.0997, 'grad_norm': 0.3960822820663452, 'learning_rate': 5.4698457223001395e-06, 'epoch': 0.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24b4800112954895ad4d902182eb6fb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2852 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.07635725289583206, 'eval_runtime': 237.1034, 'eval_samples_per_second': 96.211, 'eval_steps_per_second': 12.029, 'epoch': 1.0}\n",
      "{'train_runtime': 933.2198, 'train_samples_per_second': 24.444, 'train_steps_per_second': 0.764, 'train_loss': 0.13169578566290385, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=713, training_loss=0.13169578566290385, metrics={'train_runtime': 933.2198, 'train_samples_per_second': 24.444, 'train_steps_per_second': 0.764, 'total_flos': 1656080245555200.0, 'train_loss': 0.13169578566290385, 'epoch': 1.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "model_name = \"google/flan-t5-small\" \n",
    "model_t5 = T5ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "tokenizer_t5 = T5Tokenizer.from_pretrained(model_name, legacy=False, clean_up_tokenization_spaces=True)\n",
    "\n",
    "# Add [MASK] token if not already present\n",
    "if \"[MASK]\" not in tokenizer_t5.get_vocab():\n",
    "    tokenizer_t5.add_tokens(\"[MASK]\")\n",
    "    model_t5.resize_token_embeddings(len(tokenizer_t5))  # Resize embeddings to match the updated vocab size\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    prompt = \"Replace [MASK] to create a metaphor sentence: {masked_sentence}\"\n",
    "\n",
    "    inputs = [\n",
    "        prompt.format(masked_sentence=sentence)\n",
    "        for sentence in examples['masked_sentence']\n",
    "    ]\n",
    "    \n",
    "    targets = examples['sentence']\n",
    "    \n",
    "    model_inputs = tokenizer_t5(inputs, max_length=200, truncation=True, padding=\"max_length\")\n",
    "    labels = tokenizer_t5(targets, max_length=200, truncation=True, padding=\"max_length\")[\"input_ids\"]\n",
    "    labels = [[-100 if token == tokenizer_t5.pad_token_id else token for token in label] for label in labels]\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# dataset split to verify model works\n",
    "dataset_split = tokenized_dataset.train_test_split(test_size=0.05, seed=42)\n",
    "eval_dataset = dataset_split['test']\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./fine_tuned_flan_t5\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=3e-4,\n",
    "    per_device_train_batch_size=32,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=100,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_t5,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    eval_dataset=tokenized_dataset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_t5_combined_df/tokenizer_config.json',\n",
       " './fine_tuned_t5_combined_df/special_tokens_map.json',\n",
       " './fine_tuned_t5_combined_df/spiece.model',\n",
       " './fine_tuned_t5_combined_df/added_tokens.json')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model\n",
    "output_dir = \"./fine_tuned_t5_combined_df\" #change for each dataset trained on\n",
    "model_t5.save_pretrained(output_dir)\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer_t5.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data set to train on\n",
    "\n",
    "dataset = Dataset.from_pandas(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating mask token\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9994f6544ba489482f4034915f4948f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/22812 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00f54466d1174160b45fcebd603e79aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/713 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2081, 'grad_norm': 1.1025300025939941, 'learning_rate': 1.7194950911640955e-05, 'epoch': 0.14}\n",
      "{'loss': 0.1012, 'grad_norm': 4.234947681427002, 'learning_rate': 1.4389901823281908e-05, 'epoch': 0.28}\n",
      "{'loss': 0.0887, 'grad_norm': 0.7784596085548401, 'learning_rate': 1.1584852734922862e-05, 'epoch': 0.42}\n",
      "{'loss': 0.0774, 'grad_norm': 1.3098950386047363, 'learning_rate': 8.779803646563817e-06, 'epoch': 0.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0739, 'grad_norm': 1.0961717367172241, 'learning_rate': 5.97475455820477e-06, 'epoch': 0.7}\n",
      "{'loss': 0.069, 'grad_norm': 0.7941123247146606, 'learning_rate': 3.1697054698457223e-06, 'epoch': 0.84}\n",
      "{'loss': 0.0662, 'grad_norm': 0.9455534219741821, 'learning_rate': 3.6465638148667605e-07, 'epoch': 0.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02adba5be12b492ea8416f7ff579a4c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/143 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.039785053580999374, 'eval_runtime': 13.0881, 'eval_samples_per_second': 87.178, 'eval_steps_per_second': 10.926, 'epoch': 1.0}\n",
      "{'train_runtime': 1007.1997, 'train_samples_per_second': 22.649, 'train_steps_per_second': 0.708, 'train_loss': 0.09710398295484117, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=713, training_loss=0.09710398295484117, metrics={'train_runtime': 1007.1997, 'train_samples_per_second': 22.649, 'train_steps_per_second': 0.708, 'total_flos': 2037496301568000.0, 'train_loss': 0.09710398295484117, 'epoch': 1.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "model_name = \"facebook/bart-base\"  # Use BART model\n",
    "model_bart = BartForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "tokenizer_bart = BartTokenizer.from_pretrained(model_name, clean_up_tokenization_spaces=True)\n",
    "\n",
    "# Add [MASK] token if not already present (optional for BART)\n",
    "if \"[MASK]\" not in tokenizer_bart.get_vocab():\n",
    "    print(\"updating mask token\")\n",
    "    tokenizer_bart.add_tokens(\"[MASK]\")\n",
    "    tokenizer_bart.mask_token = \"[MASK]\"\n",
    "    model_bart.resize_token_embeddings(len(tokenizer_bart))  # Resize embeddings to match updated vocab size\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    prompt = \"Replace [MASK] to create a metaphor sentence: {masked_sentence}\"\n",
    "    \n",
    "    # Prepare inputs and targets\n",
    "    inputs = [\n",
    "        prompt.format(masked_sentence=sentence)\n",
    "        for sentence in examples['masked_sentence']\n",
    "    ]\n",
    "    targets = examples['sentence']\n",
    "    \n",
    "    # Tokenize inputs and targets\n",
    "    model_inputs = tokenizer_bart(inputs, max_length=150, truncation=True, padding=\"max_length\")\n",
    "    labels = tokenizer_bart(targets, max_length=150, truncation=True, padding=\"max_length\")[\"input_ids\"]\n",
    "    labels = [[-100 if token == tokenizer_bart.pad_token_id else token for token in label] for label in labels]\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return model_inputs\n",
    "\n",
    "# tokenize dataset\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Eval set for confirming model fit\n",
    "dataset_split = tokenized_dataset.train_test_split(test_size=0.05, seed=42)\n",
    "eval_dataset = dataset_split['test']\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./fine_tuned_bart\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32,  \n",
    "    num_train_epochs=1,\n",
    "    logging_steps=100,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_bart,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_bart_combined_df/tokenizer_config.json',\n",
       " './fine_tuned_bart_combined_df/special_tokens_map.json',\n",
       " './fine_tuned_bart_combined_df/vocab.json',\n",
       " './fine_tuned_bart_combined_df/merges.txt',\n",
       " './fine_tuned_bart_combined_df/added_tokens.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model\n",
    "output_dir = \"./fine_tuned_bart_combined_df\" #change for each dataset trained on\n",
    "model_bart.save_pretrained(output_dir)\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer_bart.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>synthetic_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The ocean [MASK] against the cliffs, roaring i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The garden [MASK] with colors as the flowers g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The airplane [MASK] through the sky, leaving a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The dog [MASK] through the field, a blur of fu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The sky [MASK] a mosaic of pink and orange as ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 synthetic_sentences\n",
       "0  The ocean [MASK] against the cliffs, roaring i...\n",
       "1  The garden [MASK] with colors as the flowers g...\n",
       "2  The airplane [MASK] through the sky, leaving a...\n",
       "3  The dog [MASK] through the field, a blur of fu...\n",
       "4  The sky [MASK] a mosaic of pink and orange as ..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_with_the = pd.read_csv(\"data/test_sentences_begin_the.csv\")\n",
    "sentences_without_the = pd.read_csv(\"data/test_sentences_no_the.csv\")\n",
    "\n",
    "sentences_with_the.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompts(sentence: str, method: str) -> str:\n",
    "    \"\"\"\n",
    "    Creates a formatted prompt for input to a generation method.\n",
    "\n",
    "    Args:\n",
    "        sentence (str): Masked sentence.\n",
    "        method (str): \"zero\" for zero-shot, \"few\" for few-shot.\n",
    "\n",
    "    Returns:\n",
    "        str: Formatted prompt for the specified method.\n",
    "    \"\"\"\n",
    "\n",
    "    # Few-shot example\n",
    "    few_shot_examples = (\n",
    "        \"Replace [MASK] to create metaphorical sentences:\\n\\n\"\n",
    "        \"Examples:\\n\"\n",
    "        \"Input: The task is [MASK] challenging.\\n\"\n",
    "        \"Output: The task is an uphill battle.\\n\\n\"\n",
    "        \"Input: She is [MASK] sad.\\n\"\n",
    "        \"Output: She is drowning in sorrow.\\n\\n\"\n",
    "        \"Return only Output:\\n\"\n",
    "    )\n",
    "\n",
    "    # Zero-shot example\n",
    "    zero_shot_example = \"Replace [MASK] to create a metaphor sentence:\"\n",
    "\n",
    "    # Create the appropriate prompt\n",
    "    if method == \"zero\":\n",
    "        prompt = f\"{zero_shot_example} {sentence}\"\n",
    "    elif method == \"few\":\n",
    "        prompt = f\"{few_shot_examples}Input: {sentence}\\nOutput:\"\n",
    "    else:\n",
    "        raise ValueError(\"Invalid method. Choose 'zero' or 'few'.\")\n",
    "\n",
    "    return prompt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replace [MASK] to create metaphorical sentences:\n",
      "\n",
      "Examples:\n",
      "Input: The task is [MASK] challenging.\n",
      "Output: The task is an uphill battle.\n",
      "\n",
      "Input: She is [MASK] sad.\n",
      "Output: She is drowning in sorrow.\n",
      "\n",
      "Return only Output:\n",
      "Input: Testing prompt and sentence structure\n",
      "Output:\n"
     ]
    }
   ],
   "source": [
    "print(prompts(\"Testing prompt and sentence structure\", \"few\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_metaphors(model, tokenizer, sentences: list[str], prompt_type: str = \"few\", view_output: bool = False, ) -> list[str]: # type: ignore\n",
    "    \n",
    "    \"\"\"\n",
    "    Generates metaphor sentences from masked sentences\n",
    "\n",
    "    Args:\n",
    "        model: pre-trained model\n",
    "        tokenizer: model tokenizer\n",
    "        sentences (list): list of sentences to transform\n",
    "        prompt_type (str): \"zero\" for zero shot \"few\" for few shot\n",
    "        view_output (bool): set true to print masked sentences and generated metaphor\n",
    "        \n",
    "    Returns:\n",
    "        A list of transformed sentences\n",
    "    \"\"\"\n",
    "    \n",
    "    model = model.to(device)\n",
    "    output = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "\n",
    "        prompt = prompts(sentence, prompt_type)\n",
    "\n",
    "        # Tokenize and move to the device\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "        # Generate the output\n",
    "        output_ids = model.generate(\n",
    "            inputs=input_ids[\"input_ids\"],  \n",
    "            max_length=150,                # Allow for longer outputs\n",
    "            num_beams=10,                  \n",
    "            do_sample=True,\n",
    "            temperature=0.5,               # Slightly higher temperature for creativity\n",
    "            #top_k=50,                      # Use top-k sampling to encourage variation\n",
    "            top_p=0.95,                     # Use nucleus sampling for creativity\n",
    "            early_stopping=False\n",
    "        )\n",
    "\n",
    "        # Decode and print the output\n",
    "        output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        output.append(output_text)\n",
    "\n",
    "        if view_output is True:\n",
    "            print(f\"Input: {sentence}\\nOutput: {output_text}\\n\")\n",
    "            \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sentences_with_the[\"synthetic_sentences\"].tolist()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: The ocean [MASK] against the cliffs, roaring its eternal song.\n",
      "Output: The ocean melted against the cliffs, roaring its eternal song.\n",
      "\n",
      "Input: The garden [MASK] with colors as the flowers greeted the sun.\n",
      "Output: The garden filled with colors as the flowers greeted the sun.\n",
      "\n",
      "Input: The airplane [MASK] through the sky, leaving a trail of whispers behind.\n",
      "Output: The airplane rolled through the sky, leaving a trail of whispers behind.\n",
      "\n",
      "Input: The dog [MASK] through the field, a blur of fur and joy.\n",
      "Output: The dog stumbled through the field, a blur of fur and joy.\n",
      "\n",
      "Input: The sky [MASK] a mosaic of pink and orange as the day turned to night.\n",
      "Output: The sky filled a mosaic of pink and orange as the day turned to night.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['The ocean melted against the cliffs, roaring its eternal song.',\n",
       " 'The garden filled with colors as the flowers greeted the sun.',\n",
       " 'The airplane rolled through the sky, leaving a trail of whispers behind.',\n",
       " 'The dog stumbled through the field, a blur of fur and joy.',\n",
       " 'The sky filled a mosaic of pink and orange as the day turned to night.']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5_output = generate_metaphors(model=model_t5,tokenizer=tokenizer_t5, sentences=sentences, prompt_type=\"zero\", view_output=False)\n",
    "t5_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: The ocean [MASK] against the cliffs, roaring its eternal song.\n",
      "Output: The ocean rained against the cliffs, roaring its eternal song.\n",
      "\n",
      "Input: The garden [MASK] with colors as the flowers greeted the sun.\n",
      "Output: The garden filled with colors as the flowers greeted the sun.\n",
      "\n",
      "Input: The airplane [MASK] through the sky, leaving a trail of whispers behind.\n",
      "Output: The airplane drags through the sky, leaving a trail of whispers behind.\n",
      "\n",
      "Input: The dog [MASK] through the field, a blur of fur and joy.\n",
      "Output: The dog drags through the field, a blur of fur and joy.\n",
      "\n",
      "Input: The sky [MASK] a mosaic of pink and orange as the day turned to night.\n",
      "Output: The sky turned a mosaic of pink and orange as the day turned to night.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['The ocean rained against the cliffs, roaring its eternal song.',\n",
       " 'The garden filled with colors as the flowers greeted the sun.',\n",
       " 'The airplane drags through the sky, leaving a trail of whispers behind.',\n",
       " 'The dog drags through the field, a blur of fur and joy.',\n",
       " 'The sky turned a mosaic of pink and orange as the day turned to night.']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart_output = generate_metaphors(model=model_bart,tokenizer=tokenizer_bart, sentences=sentences, prompt_type=\"zero\", view_output=True)\n",
    "bart_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maui",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
